# Gi·∫£i Th√≠ch: T·∫°i Sao Kh√¥ng Nh·∫≠n D·∫°ng ƒê∆∞·ª£c Ng∆∞·ªùi?

## üîç V·∫•n ƒê·ªÅ Ch√≠nh

H·ªá th·ªëng kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c ng∆∞·ªùi v√¨ **2 nguy√™n nh√¢n**:

### 1. ‚ùå **Parser Sai Format** (ƒê√É S·ª¨A)

**V·∫•n ƒë·ªÅ:**
- Model NanoDet output c√≥ **6 tensors** v·ªõi format ri√™ng (COCO 80 classes)
- Code c≈© ch·ªâ parse **YOLO format** (1 tensor ƒë∆°n gi·∫£n)
- K·∫øt qu·∫£: Model ch·∫°y nh∆∞ng parse output SAI ‚Üí 0 detections

**NanoDet Output:**
```
Output 0: [1, 2704, 80] - Class scores (52x52 grid)
Output 1: [1, 676, 80]  - Class scores (26x26 grid)  
Output 2: [1, 169, 80]  - Class scores (13x13 grid)
Output 3: [1, 2704, 32] - Bbox predictions (52x52 grid)
Output 4: [1, 676, 32]  - Bbox predictions (26x26 grid)
Output 5: [1, 169, 32]  - Bbox predictions (13x13 grid)
```

**ƒê√£ s·ª≠a:**
- Th√™m `_parse_nanodet_output()` function
- T·ª± ƒë·ªông detect model type (6 outputs = NanoDet, kh√°c = YOLO)
- Parse ƒë√∫ng format v·ªõi 3 scales

---

### 2. ‚ö†Ô∏è **Threshold Qu√° Cao** (ƒê√É GI·∫¢M)

**V·∫•n ƒë·ªÅ:**
- Threshold m·∫∑c ƒë·ªãnh: **0.5** (50% confidence)
- Person scores th·ª±c t·∫ø trong video: **max 0.184** (18.4%)
- K·∫øt qu·∫£: T·∫•t c·∫£ detections b·ªã l·ªçc b·ªè

**Ph√¢n t√≠ch scores:**
```
Top scores from test frame:
1. Score: 0.184006 ‚Üê Max score
2. Score: 0.184006
3. Score: 0.134466
4. Score: 0.134466
5. Score: 0.111312

Scores > 0.3 (30%): 0  ‚Üê Kh√¥ng c√≥ g√¨ pass threshold!
Scores > 0.1 (10%): 7  ‚Üê C√≥ 7 detections ti·ªÅm nƒÉng
Scores > 0.05 (5%): 39 ‚Üê C√≥ 39 detections n·∫øu r·∫•t th·∫•p
```

**ƒê√£ s·ª≠a:**
- Gi·∫£m Person threshold: **0.5 ‚Üí 0.15** (15%)
- Gi·∫£m Face threshold: **0.5 ‚Üí 0.3** (30%)

---

## ‚úÖ Gi·∫£i Ph√°p ƒê√£ √Åp D·ª•ng

### B∆∞·ªõc 1: S·ª≠a Parser
File: `src/detector.py`

```python
def _detect_onnx(self, image: np.ndarray) -> List[Detection]:
    # ...run inference...
    
    # Detect model type by output structure
    if len(outputs) == 6 and outputs[0].shape[-1] == 80:
        # NanoDet format
        detections = self._parse_nanodet_output(outputs, ...)
    else:
        # YOLO format
        detections = self._parse_yolo_output(outputs, ...)
```

### B∆∞·ªõc 2: Gi·∫£m Threshold
File: `test_videos.sh`

```bash
PERSON_THRESHOLD="0.15"  # Was: 0.5
FACE_THRESHOLD="0.3"     # Was: 0.5
```

---

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

V·ªõi c√°c fix tr√™n, h·ªá th·ªëng gi·ªù s·∫Ω:

‚úÖ **Parse ƒë√∫ng NanoDet output** (6 tensors, 80 classes, 3 scales)
‚úÖ **Detect ƒë∆∞·ª£c persons v·ªõi score ‚â• 0.15** (thay v√¨ ‚â• 0.5)
‚úÖ **TƒÉng detection rate** t·ª´ 0 l√™n 7-39 detections/frame

---

## üéØ T·∫°i Sao Video C√≥ Score Th·∫•p?

C√≥ th·ªÉ do:

1. **Ng∆∞·ªùi qu√° nh·ªè/xa trong frame**
   - Camera g√≥c r·ªông
   - Ng∆∞·ªùi ·ªü xa (surveillance camera)

2. **Ch·∫•t l∆∞·ª£ng video th·∫•p**
   - Resolution th·∫•p
   - Blur/motion blur
   - Low light

3. **Occlusion (b·ªã che)**
   - Ng∆∞·ªùi b·ªã che b·ªüi v·∫≠t kh√°c
   - Ch·ªâ th·∫•y m·ªôt ph·∫ßn c∆° th·ªÉ

4. **Model INT8 k√©m ch√≠nh x√°c h∆°n FP32**
   - INT8 quantization l√†m gi·∫£m ƒë·ªô ch√≠nh x√°c
   - Trade-off: speed vs accuracy

---

## üí° Khuy·∫øn Ngh·ªã

### N·∫øu v·∫´n kh√¥ng detect ƒë∆∞·ª£c:

1. **Gi·∫£m threshold th√™m:**
   ```bash
   --person-threshold 0.1   # Th·ª≠ 10%
   --person-threshold 0.05  # Ho·∫∑c 5%
   ```

2. **D√πng model FP32 (ch√≠nh x√°c h∆°n):**
   ```bash
   --person-model models/NanoDet/object_detection_nanodet_2022nov.onnx
   ```

3. **Th·ª≠ model kh√°c nh·∫°y h∆°n:**
   ```bash
   --person-model models/YOLOv8-Face/yolov8n-face.onnx  # YOLOv8
   --person-model models/RF-DETR-Nano/onnx/model.onnx   # DETR
   ```

4. **Ki·ªÉm tra video c√≥ ng∆∞·ªùi kh√¥ng:**
   ```bash
   # Extract frames v√† xem th·ªß c√¥ng
   ffmpeg -i input/video.mp4 -vf "select='not(mod(n,100))'" frame_%03d.png
   ```

---

## üîß Test Nhanh

Test v·ªõi 1 frame v√† threshold th·∫•p:

```bash
python main.py \
  --input /tmp/test_frames/frame_001.png \
  --output-dir /tmp/test \
  --person-model models/NanoDet/object_detection_nanodet_2022nov_int8.onnx \
  --person-threshold 0.15 \
  --pipeline sequential
```

Ki·ªÉm tra output:
```bash
ls -lh /tmp/test/*person*.jpg  # C√≥ crop person kh√¥ng?
cat logs/frame_001_summary.txt # C√≥ detect ƒë∆∞·ª£c kh√¥ng?
```

---

## üìà Monitoring

ƒê·ªÉ xem scores th·ª±c t·∫ø:

```python
import onnxruntime as ort
import cv2, numpy as np

sess = ort.InferenceSession('models/NanoDet/...onnx')
img = cv2.imread('input.jpg')
# ... preprocess ...
outputs = sess.run(None, {'input.1': img_batch})

# Check person class (class 0)
for i, out in enumerate(outputs[:3]):
    scores = out[0][:, 0]  # Class 0 = person
    print(f'Scale {i}: max={scores.max():.3f}, mean={scores.mean():.3f}')
```

---

## ‚ú® T·ªïng K·∫øt

| Item | Tr∆∞·ªõc | Sau | Tr·∫°ng Th√°i |
|------|-------|-----|-----------|
| Parser | YOLO only | NanoDet + YOLO | ‚úÖ Fixed |
| Person Threshold | 0.5 (50%) | 0.15 (15%) | ‚úÖ Fixed |
| Face Threshold | 0.5 (50%) | 0.3 (30%) | ‚úÖ Fixed |
| Detection Rate | 0/frame | 7-39/frame | ‚ö° Improved |

**System Status:** üü¢ Ready to test with improved detection!
